# **Temperature Anomaly Prediction: Influencing Factors and Modeling Strategies**







## **Part 1: Influencing Factors on Temperature Anomaly**





**Understanding Temperature Anomalies:** A daily temperature *anomaly* is the difference between a day’s actual average temperature and the long-term climatological average for that date. Such anomalies arise when atmospheric conditions deviate from the seasonal norm. We can categorize the drivers of these deviations by scale – from global climate patterns, to synoptic (continental/regional) weather systems, down to local geographic effects. Below, we identify key factors at each scale and explain how they can cause a day’s mean temperature to stray above or below its 7-year normal, focusing on those whose signatures might be captured by hourly station data.





### **Global-Scale Climate Phenomena**





*Global patterns like the polar vortex influence mid-latitude temperature anomalies. A strong, stable polar vortex (left) locks cold air in the Arctic, while a weakened or disrupted vortex (right) allows frigid air to spill southward, causing cold anomalies in mid-latitudes.*



-   **El Niño–Southern Oscillation (ENSO):** ENSO is a tropical Pacific ocean-atmosphere cycle with global reach. **El Niño** events (warm-phase ENSO) release excess ocean heat to the atmosphere, often raising global temperatures and altering jet stream paths . In East Asia, El Niño tends to weaken the East Asian winter monsoon, bringing milder, warmer-than-normal winters (as seen in the very warm winter of 2023–24)  . Conversely, **La Niña** (cool-phase ENSO) usually strengthens the winter monsoon, allowing more cold Arctic air outbreaks and below-normal winter temperatures in Korea and nearby regions  . These ENSO-driven shifts in atmospheric circulation can cause seasonal temperature anomalies – e.g. extended warm spells or cold spells relative to climatology. While our station dataset may not explicitly include ENSO indices, we can proxy these effects via year and month features (since, for example, the winters of strong El Niño years are generally warmer in Korea).
-   **Polar Vortex & Arctic Oscillation:** The polar vortex is a persistent whirl of cold air around the pole; its strength is linked to the Arctic Oscillation (AO), an index of hemispheric pressure patterns. When the stratospheric polar vortex is *strong and stable*, the polar jet stream stays farther north with a more zonal (west-east) flow. This keeps Arctic air confined to high latitudes and often yields milder-than-usual mid-latitude weather (a positive AO phase) . In this scenario, mid-latitude locations like Korea may experience fewer extreme cold days than normal for that season. In contrast, if the polar vortex *weakens or disrupts* (negative AO), the jet stream becomes wavy and can buckle far south. This allows lobes of cold polar air to plunge into mid-latitudes while warmer air moves north . Such events cause sharp cold anomalies (e.g. severe cold snaps well below the climatological mean) when Arctic air masses invade lower latitudes. For instance, a sudden stratospheric warming event that disrupts the vortex can trigger prolonged cold outbreaks in Eurasia. Though our data doesn’t measure the vortex directly, the resulting cold-air outbreak would be evident in station readings (e.g. an abrupt temperature drop alongside rising pressure and a wind shift from north). Including large-scale pressure pattern indicators (like an Arctic Oscillation index if available, or simply the presence of extreme cold advection in the data) can help capture this influence.
-   **Background Climate Trends:** Over a span of 2018–2024, there is a slight warming trend from global climate change. While a 7-year climatology partially reflects recent climate, any ongoing trend could make recent years systematically warmer than the 7-year average (creating positive anomalies). For example, globally 2023 was the warmest year on record, fueled by El Niño on top of greenhouse warming  . In practice, this means later years in the data might see more frequent positive anomalies. We can account for this by including a time index or year feature in the model. Although not a short-term “cause” of daily anomalies, the long-term trend sets the baseline – raising the odds that, say, a moderately warm day now counts as above “normal” relative to a cooler past climatology.







### **Synoptic-Scale Weather Influences**





These are continental- or regional-scale weather systems (spanning a few hundred to a thousand kilometers) that can significantly elevate or depress temperatures for a day or a few days. Station data often contains clues about these systems, such as pressure changes or wind shifts.



-   **Jet Stream Position and Pressure Systems:** The undulations of the polar jet stream and the arrangement of high- and low-pressure systems are major drivers of day-to-day temperature anomalies. When a **high-pressure ridge** builds over a region, it often brings subsiding air and clear skies. This can lead to anomalous warmth, especially in warmer months – a classic heat wave scenario. The sinking air under a blocking anticyclone compresses and warms the lower atmosphere, creating a “dome” of heat . With minimal cloud cover, excessive solar heating by day and reduced cooling at night yield temperatures far above the climatological mean (heat wave anomalies). In East Asia’s summer, a stagnant subtropical high can cause an extended hot spell above normal. Conversely, a **deep trough** in the jet stream or a strong cold-core low can draw down polar air, causing unseasonably cool days. For example, a surge of the Siberian air mass associated with the East Asian winter monsoon can bring temperatures well below normal in Korea. A dominant **Siberian High** pressure over Mongolia/Manchuria with northwesterly winds is known to advect bitter cold, dry air into Korea during winter  – resulting in cold anomalies (often accompanied by clear, frigid conditions). In our dataset, such synoptic situations could be inferred from patterns like high sea-level pressure, winds from the northwest, and low dew points. Features capturing pressure tendencies (falling pressure might herald an approaching cyclone/warm front; rising pressure often follows a cold front) or wind direction frequencies can serve as proxies for these synoptic setups.
-   **Air Mass Advection (Warm/Cold Fronts):** Anomalies often occur when an atypical air mass visits a region. A **warm front** or prolonged southerly flow can flood a normally cool area with warm air (e.g. a late-winter day turning out much warmer than average due to subtropical air). On the flip side, a **cold front** can introduce a stark cold-air mass, making a day colder than usual. These effects can be short-lived but dramatic. For instance, if one day’s observations show a sudden temperature drop in the afternoon along with a wind shift from southwest to northeast and rising barometer, that likely indicates a cold frontal passage bringing colder-than-normal air for the next day. Likewise, an unusually strong foehn wind (downslope dry wind) could elevate local temperatures (as sometimes observed on Korea’s east coast when westerly winds descend mountains). **Tropical cyclones (typhoons)** or other storms can also affect anomalies: a passing typhoon in summer might bring thick clouds and rain that suppress daytime highs below normal (cool anomaly), or in some cases draw in warm tropical air causing a warm night. The key synoptic drivers – fronts, cyclones, and anticyclones – all leave signatures in the hourly data (e.g. wind direction changes, pressure jumps, precipitation onset). By engineering features like “24-hr pressure change” or counts of wind from northerly directions, we enable the model to infer when an unusual air mass advection is occurring, which correlates with next-day temperature departures.
-   **Jet Stream & Upper-Level Dynamics:** Although upper-atmosphere data isn’t in our ground station dataset, its effects trickle down. A highly meridional (north-south wavy) jet stream often coincides with blocking highs and cut-off lows that linger. These patterns cause multi-day anomalies: e.g. a stalled upper low can keep it cooler and cloudier than normal for an extended period. A strong jet stream zonal flow, by contrast, means quick-moving weather and temperatures closer to normal. Such concepts are harder to capture from surface data alone, but prolonged anomalies might be identified by persistence features (e.g. multi-day runs of similar temperature error). In practice, many synoptic influences overlap with what the station data can tell us about air mass origin (via wind and humidity) and atmospheric stability (via pressure and cloud/precip indicators).







### **Local-Scale Effects**





Local geography and surface conditions can modulate temperatures enough to create anomalies relative to a station’s own climatology. Since our model will learn per-station patterns, capturing these local effects via features is important.



-   **Urban Heat Island (UHI):** Urban areas tend to be warmer than their rural surroundings due to human-made surfaces and reduced vegetation. Cities heat up more during the day and retain heat at night. As a result, an urban station’s nighttime minimum can be much higher than the climatological expectation based on a broader region. The urban heat island effect can raise **daytime** temperatures by 1–7 °F and **nighttime** temperatures by 2–5 °F compared to rural areas . Thus, an urban station might frequently see positive anomalies, especially overnight, when rural areas would cool more. For example, on a clear, calm night, a city center might stay significantly warmer than a typical “average” night, registering a warm anomaly, whereas a rural station might drop to its normal minimum. UHI effects are mostly constant (built into a station’s normal climate), but they can spike during certain conditions – e.g. during heat waves or under stagnant air, urban areas heat disproportionately. To approximate this, we can include metadata such as station type or population (if available) or simply a station identifier so that the model can learn each station’s typical bias and variance. If station IDs are included as features (or used in a grouping scheme), the model can implicitly account for an urban station’s tendency to run warm. Additionally, features like “last night’s min temp” relative to a rural reference could flag UHI intensity, but that requires more data. In summary, while UHI is a known local cause for warmer anomalies, we handle it in modeling by treating each station individually (and possibly by adding features for land use if we infer them from station names).
-   **Topography (Elevation and Terrain):** Elevation and local terrain features can cause microclimate effects. Higher elevation stations are generally cooler, but their climatology accounts for that. More relevant is how topography affects temperature *relative to normal* on a given day. **Valley Inversions:** At night, especially in cold seasons under clear skies, cold dense air drains into valleys. This can make valley stations much colder than hilltop stations – sometimes colder than their normal expectation if an unusually strong inversion sets up. Conversely, if on a particular night winds prevent an inversion that normally forms, a valley might stay warmer than its climatological norm (a positive anomaly). In other words, variability in inversion strength can create anomalies. Cold-air pooling in valleys means lower elevations can occasionally be colder than higher elevations, defying the usual lapse rate . If our dataset has multiple stations, a feature capturing relative temperature differences between a valley station and a nearby higher station could indicate an inversion, but if not, we might infer it from conditions (clear, calm, high pressure nights favor strong cooling). **Mountain Slopes:** A station on a slope might be exposed to sun or wind differently on certain days, causing departures. **Coastal Influence:** Stations near the ocean have smaller diurnal ranges and a maritime moderation. If an atypical wind pattern occurs (e.g. an offshore wind in summer cutting off the sea breeze), a coastal city can get hotter than normal; if an onshore breeze persists on a day that is usually hot, it could be cooler than normal. Thus, coastal anomalies are often tied to wind direction. We can use each station’s location (coastal vs inland flag, or even latitude/longitude if available) and wind data to help the model learn these effects. For example, an algorithm might learn that a strong west wind (from the ocean) at a Busan station usually means cooler-than-average daytime temperature for that date (negative anomaly), whereas an east wind (downslope off mountains) means a warm spike.
-   **Snow Cover and Surface Wetness:** Surface conditions like snow and soil moisture can dramatically alter local temperature responses. **Snow Cover** in particular tends to *cool* the environment relative to normal. A fresh snowpack has a very high albedo (reflecting 80–90% of sunlight) , so sunny energy that would normally warm the ground and air is instead reflected away. The result: daytime temperatures stay lower than usual when snow is on the ground . For instance, if an early-season snow falls in autumn, the next day’s high might be much colder than the climatological average for that date because the sun’s energy goes into melting/reflecting rather than heating. At night, snow cover also encourages strong radiative cooling (and insulates the ground heat) so nights can be colder . Therefore, a snow-covered surface often yields a cold anomaly until the snow melts. Our hourly data likely reports precipitation type or at least temperature and precipitation – from which we can infer snow events (e.g. if temperature is near or below 0°C during precipitation). We can engineer a **snow flag** feature: if it snowed today or there is lingering snow (perhaps deduced if precipitation fell with sub-freezing temps), then tomorrow’s anomaly might skew cold. Similarly, **soil moisture or rainfall**: a very wet ground from heavy rain can slightly moderate temperatures (evaporation uses energy, reducing heating). Also, rain typically comes with clouds; a rainy day can be cooler than normal (clouds blocking sun) but a rainy night can be warmer than normal (cloud blanket). We can include a binary “rain happened” feature or even the total precipitation as a feature. This event flag will help the model learn patterns like “if it rained today, tomorrow’s max might be a bit lower” or “if skies were clear (no precip) and pressure high, expect large nocturnal cooling.”
-   **Local Land Use and Vegetation:** Areas with different land cover (forests, urban, agricultural) respond differently to weather. For example, rural vegetated areas cool more at night (through evapotranspiration) than urban centers. Large bodies of water (lakes, reservoirs) near a station can also buffer temperature swings. While we may not have explicit data for each station’s surroundings, the station identifier or name might hint at it (e.g. a station named “Mountain Park” vs “Downtown”). We could derive a simple categorical feature for station environment if known. In absence of that, again, treating each station separately in the model training (or including coordinates which a model can correlate with coastal or inland climate) is our way to handle these fixed local influences.





**Connecting Factors to Data:** It’s important to note that many large-scale influences manifest through local weather variables. For instance, a global phenomenon like El Niño influences Korea’s weather via altered pressure and wind patterns, which would be reflected in our station observations (perhaps through a milder winter overall, which our model can capture via seasonal trends or year indicators). Synoptic systems – highs, lows, fronts – directly show up in the hourly data (temperature swings, wind shifts, pressure changes, precipitation). Local effects can often be inferred from station metadata or by comparing a station’s readings to expectations (e.g. detecting an inversion from unusual vertical temperature differences, if multiple stations are available). In summary, by crafting features from the hourly data that reflect these phenomena (such as “today’s pressure trend” for synoptic-scale, or “station elevation” for local-scale), we enable a regression model to leverage domain knowledge of *why* a day might be warmer or cooler than normal.





## **Part 2: Feature Engineering & Modeling Strategies**





Using the rich hourly observations (2018–2024) from ground stations in South Korea, we now turn to how to effectively construct features and choose modeling approaches to predict the next day’s temperature anomaly. The goal is to design a winning strategy as one might in a Kaggle competition – leveraging both practical machine learning techniques and meteorological insights. We outline recommendations in three areas: **(1) Feature Engineering**, **(2) Modeling Approaches**, and **(3) Interpretability methods**, with an emphasis on approaches that are feasible with the given data and that make physical sense.





### **1. Feature Engineering**





To predict tomorrow’s temperature anomaly, we can derive numerous features from today’s (and recent days’) hourly weather data and any available metadata. Key categories of features include temporal aggregates, frequency-domain transforms, time-deltas, physical derivations, event flags, and spatial context:



-   **Daily Aggregates (Summary Statistics):** It is useful to condense the 24 hours of weather data into descriptive statistics for the day. For each station-day, we can compute features like the daily **mean temperature**, **maximum temperature**, **minimum temperature**, and **diurnal range** (max–min). These capture the overall thermal profile of the day. Similarly, we can take the **standard deviation** of temperature over the day, which indicates variability (a high std dev might mean a big day-night swing or a frontal passage). Other weather variables can be aggregated too: e.g. daily mean humidity, total precipitation, average wind speed, etc. These aggregates provide a first-order summary that is often predictive of the next day’s conditions. For example, an unusually high max temperature today (relative to normal) might suggest a warm air mass is in place, which could carry over to tomorrow’s anomaly. A low minimum might indicate strong radiative cooling which, if the pattern persists, could mean a cold anomaly trend. Aggregates also smooth out noise in hourly data and are straightforward for models to use. Many winning Kaggle solutions begin by creating such statistical features for each day. We should be careful to normalize or detrend these by date if needed (since the model must distinguish an 18°C max in January as anomalously warm, whereas 18°C in July is cool). However, since our target is anomaly (already relative to date norm), raw values as features are acceptable. In practice, including today’s mean, min, max temperatures (and perhaps yesterday’s as well) gives the model a sense of persistence and current departure from normal.
-   **Signal Transforms (FFT/DCT of Hourly Series):** To capture the *shape* of the diurnal cycle and any sub-daily patterns, we can apply frequency transformations on the hourly data. By taking a **Discrete Fourier Transform (DFT)** or **Discrete Cosine Transform (DCT)** of the 24-hour temperature series, we extract coefficients representing the dominant frequencies. For example, the first harmonic (24-hour cycle) amplitude indicates how large the day-night swing was; the second harmonic (12-hour) might capture midday vs midnight differences, etc. These coefficients can serve as compact features encoding the day’s temperature curve. If the dataset exhibits characteristic patterns (like perhaps a bimodal temperature distribution on days with sea breezes vs without), the transform may highlight that. Another approach is to use a small set of **principal components** or a fixed set of basis curves (sines/cosines) to represent the hourly profile. **Why use these?** Meteorologically, the diurnal cycle amplitude is related to cloud cover and humidity – a low amplitude (small difference between day and night temps) often means cloudy or humid conditions, which could influence whether tomorrow is cooler or warmer than normal (e.g. a cloudy day today might mean a system is in place, affecting tomorrow). Likewise, unusual spikes or plateaus in the hourly series (captured by higher-frequency components) might flag transient events (like a front at 3pm causing an abrupt temp drop). By including FFT/DCT features, the model can learn associations like “a strong 24-hr cycle today (clear skies) often precedes a big cool-off at night, affecting anomaly” or “if the temperature doesn’t drop much overnight (low-frequency component indicating warm night), tomorrow’s morning is already warm”. These features were successfully used in some Kaggle time-series competitions to encode periodic patterns. We must ensure not to leak information from future hours of the target day – but since we only use the current day’s 24-hour series to predict next day, we’re safe.
-   **Time-Based Trends and Differences:** Weather has autocorrelation – a warm anomaly today often correlates with a warm anomaly tomorrow unless a regime change occurs. We should include **lag features** and **day-to-day differences** to capture these trends. For example, the **difference in today’s mean temperature vs yesterday’s mean** (ΔT_mean) tells us if a warming or cooling trend is in progress. If today was much warmer than yesterday (positive ΔT), tomorrow might also lean warmer than normal (persistence of warm air mass) until a front breaks the pattern. Similarly, differences in pressure (today’s mean sea-level pressure minus yesterday’s) can indicate an approaching low or high. We can include **yesterday’s anomaly** itself as a feature – essentially a one-step persistence model inside our model. Often tomorrow’s anomaly is correlated with today’s anomaly (especially for slow-changing phenomena like ENSO-driven patterns or multi-day heatwaves/coldwaves). By giving the model the previous day’s anomaly or previous day’s deviation from normal, it can learn that autocorrelation. We just must compute it carefully only from past data (which in training we can, and in inference, we’d have yesterday’s actual temperature so yes). Another valuable trend feature: **rolling averages** or **moving window trends**. For example, a 3-day moving average temperature or a 7-day trend can capture slow swings (if a week has been steadily colder than normal, the ground might be cooler, etc.). **Day-of-week or holiday effects** are not relevant to weather per se (unlike say energy demand forecasting), so we can ignore those. But **seasonal timing** is – however, seasonal normal is already part of anomaly definition, so including, say, “day of year” might not add much for the anomaly target (except to help with any remaining seasonal pattern or climatology drift). The main idea is to let the model know whether we are currently in an upswing or downswing relative to normal. These delta features are easy to compute: e.g., temp_mean_diff_1 = today_mean_temp - yesterday_mean_temp. If our data has multiple years, we can also incorporate an anomaly from a year ago (though climate change might make that less useful). In practice, for Kaggle-style time series, lag features (yesterday’s values, last week’s values) often boost accuracy because they provide context of recent momentum. We should use **Group-based differencing by station**, meaning the diff is taken station-wise (since each station has its own sequence).
-   **Derived Physical Features:** We can create features that have physical meaning, which often generalize better. One such feature is the **dew point depression** (temperature minus dew point, at a representative time or averaged). This measures how dry the air is. A small dew point depression (temp ≈ dew point) means humid air or near-saturation, which usually implies clouds or fog and less nighttime cooling. A large depression means very dry air, allowing strong radiative cooling at night and large diurnal swings. For example, if in the late afternoon the dew point is far below the temperature, you can expect a rapid temperature drop after sunset – potentially leading to a cooler-than-normal night (cold anomaly overnight). By including **(T - Td)** or even the min dew point or max dew point of the day, we give the model a handle on moisture conditions. Another useful derived feature is **relative humidity** or **saturation fraction at coldest hour** – essentially also capturing moisture’s effect on lows. **Wind-derived features:** We should break wind into its components (e.g. **u = cos(direction)*speed, v = sin(direction)speed* to get eastward and northward wind components). This allows the model to learn that, say, a positive v-component (southerly wind) often yields warming. Or we could include categorical wind direction bins (N, NE, E, etc.) for prevailing wind at certain hours. For instance, an *average wind direction from the northwest* today might be a proxy for cold advection, suggesting a cold anomaly. **Pressure-derived features:** Sea-level pressure or station pressure can be very telling. We might include **today’s mean pressure**, **today’s min pressure**, etc. A very low pressure (far below normal for that region/date) usually accompanies a storm system – possibly meaning clouds and cooler temps, or if it’s a warm-core typhoon remnant, maybe warmer night. High pressure suggests clear skies and more extreme diurnal range. **Temperature gradients:** If we have multiple stations, one can derive features like the difference between a coastal station’s temp and an inland station’s temp as a surrogate for sea breeze strength or regional front presence. However, if modeling each station separately, that gets complicated – more feasible would be to use external reanalysis, but since we stick to given data, that’s beyond scope. **Other meteorological indices:** If the data includes cloud cover or solar radiation, those would be extremely useful derived features (like daily total solar energy, which indicates how sunny it was). If not, one could infer cloudiness from the diurnal temp range (small range implies clouds). So perhaps include **diurnal range** as mentioned, which doubles as a cloudiness indicator. In summary, by deriving these physically meaningful features (humidity measures, wind components, pressure changes), we imbue the model with meteorological reasoning. These features can improve generalization because they hold consistent meaning across stations and times (e.g. 10 hPa pressure drop in a day likely means a frontal passage no matter where you are, which likely impacts anomalies in a predictable way).
-   **Event Flags (Precipitation, Snow, Storms):** Binary flags for certain weather events can be very powerful, as they introduce non-linearity in a simple way (event happened vs not). We should create a **precipitation flag** that is 1 if any measurable precipitation fell during the day (and perhaps the amount as a separate feature). Precipitation indicates cloud cover and often an associated front or low pressure. The presence of rain usually leads to cooler daytime temps (clouds) and warmer nighttime temps (cloud insulation) than otherwise – affecting the daily average in complex ways. But overall, a rainy spell might mean a certain synoptic regime (e.g. monsoonal flow). Including a rain flag allows the model to adjust predictions: for example, if today was rainy (which might have made today cooler than normal), what does that imply for tomorrow? It could mean the system is passing and tomorrow might rebound warmer, or if the rain continues, tomorrow stays cool. The model can figure out the correlation. Similarly, a **snow flag** is crucial as discussed: if it snowed (especially if snow remains on ground), tomorrow is likely colder than normal (due to high albedo and continued cold air). We can infer snow from temperature and precipitation (e.g. flag = 1 if precip > 0 and temp_min < 0°C). Another event is **frost** (temp below 0 with clear sky) – but that overlaps with snow/cold. **Thunderstorm** could be flagged if the data has weather codes; a storm might temporarily cool but then post-storm warming could occur. For simplicity, rain/snow are the main ones. We might also create a **fog or low-visibility flag** if data permits (fog often means very little solar heating, so cool day, or it could mean trapped cold air). If the station data has any categorical weather observations (like “rain”, “snow”, “fog”, etc.), we should one-hot encode those for the day. These event flags turn qualitative shifts in weather into features the model can use.
-   **Spatial/Geographic Features:** While each station’s anomaly is relative to its own normal, adding spatial context can help the model discern regional patterns and handle new data. One approach is to include the **station identity** as a categorical feature. This allows a model like XGBoost or a neural net (with embedding) to learn station-specific offsets or sensitivities. For example, it might learn that “Station A” tends to have larger swings than “Station B,” and adjust predictions accordingly. However, including station ID in a regression can lead to overfitting if not careful (the model might memorize each station’s outcomes). If the competition rules allow it, one could also incorporate **latitude, longitude, and elevation** of each station as numeric features. These provide a continuous way to represent location. The model could then infer, for instance, that coastal stations (which might have lower latitude or particular lon/lat combos) have smaller anomalies, or that higher elevation stations respond differently to weather events. We could add an **“is_coastal” flag** derived from coordinates or station name (if the name is a coastal city or an island). Another spatial feature is grouping stations by climate region (e.g. northwest inland, southeast coast, mountain, urban metro, etc.) if such classification is known – essentially a higher-level category than individual station. This might help if certain regions respond similarly to synoptic conditions. If the dataset doesn’t directly provide these, we can infer some (e.g. station name contains “airport” often implies open area, station name contains city vs county, etc.). **Neighbor station influences:** In some models, one might include features from nearby stations (spatial lag), but that can complicate things and might not be allowed if each station’s prediction must be independent. Given a Kaggle scenario, it’s safer to treat each station separately but consistently. Including station location features will also help the model extrapolate to any new station if needed by understanding how geography affects anomaly (though in our 2018–2024 data, maybe no “new” station, but still good practice).





By engineering this rich set of features – statistical, spectral, trend-based, physical, event-driven, and spatial – we equip our model with a comprehensive view of the factors discussed in Part 1. Each feature has a rationale (from domain knowledge) for why it could help predict the anomaly. In a competition setting, one would generate many of these and then possibly do feature selection based on importance or cross-validation performance to avoid overfitting on spurious ones.





### **2. Modeling Approaches**





With a diverse feature set in hand, we can experiment with various regression modeling techniques. Our goal is to capture the relationships between today’s weather features and tomorrow’s temperature anomaly in a robust, generalizable way. We also need to consider the structure of the data (multiple stations, time series) when training models and validating performance. Here are recommended modeling approaches:



-   **Regularized Linear Models (Ridge, Lasso, ElasticNet):** A linear regression model makes predictions as a weighted sum of features, which is a reasonable starting point given many features have roughly linear relationships with the next day’s anomaly. Applying regularization (shrinkage of coefficients) is crucial to avoid overfitting given the high-dimensional feature space we likely have. **Ridge regression** (L2 regularization) adds a penalty on the squared magnitude of coefficients, reducing overfitting and keeping all features but with smaller weights. **Lasso regression** (L1 regularization) adds a penalty on absolute coefficients, which can zero-out some weights entirely, performing feature selection. **ElasticNet** combines both penalties. These models are fast and interpretable – they provide a direct sense of feature importance via coefficients, and the regularization ensures the model doesn’t memorize noise. For instance, Lasso might discover that only a subset of our 100+ engineered features truly matter and set the rest to zero. Regularized linear models have the advantage of **simplicity and low variance** – they won’t overshoot wildly on unseen data if properly tuned, and they can extrapolate linearly. They also naturally handle multicollinearity by distributing weight or dropping redundant features. In a Kaggle context, a linear model can serve as a solid baseline and sometimes part of an ensemble (since it will capture linear effects that perhaps tree models might miss if not enough depth). As an example, a Ridge regression could learn that “every 1°C increase in today’s max temp tends to add 0.X°C to tomorrow’s anomaly” with some coefficient X, but regularization will temper that effect if it’s not consistently true across the dataset . Overall, we should include at least one linear model in our approach, tuning the regularization parameter (α or λ) via cross-validation. Ridge/Lasso are known to improve predictive performance by controlling overfitting , which is valuable given potential noise in meteorological data.
-   **Tree-Based Ensemble Methods (Gradient Boosted Trees):** Non-linear models like **XGBoost**, **LightGBM**, or **CatBoost** are a staple in Kaggle tabular competitions. These algorithms build ensembles of decision trees and can capture complex interactions between features. For our problem, a tree-based model can automatically discover rules such as “if wind is northerly and pressure rose, then predict a big negative anomaly” or interactions like “a high dew point matters only if temperature was low,” which a linear model might miss. **XGBoost** and **LightGBM** are particularly powerful and fast implementations of gradient boosting. They can handle large numbers of features and data points efficiently, and they inherently perform feature selection by splitting on the most informative features first. We will need to carefully tune hyperparameters like the number of trees, max depth, learning rate, and regularization terms, as these models can otherwise overfit. Hyperparameter tuning can be done with techniques like grid search or Bayesian optimization (e.g. using Optuna). For example, setting a modest tree depth (maybe 5-8) might be sufficient to capture interactions like season × pressure × station without overfitting. Boosted trees are adept at modeling non-linear relationships; if the effect of a feature on anomaly is not constant but depends on another feature (like humidity’s effect depends on temperature), the tree can branch accordingly. These methods have *proven success in many Kaggle competitions* due to their ability to handle diverse data types and complex interactions efficiently . In fact, XGBoost became famous for being the algorithm behind many winning solutions. In our context, LightGBM (which is similar) might train faster with large data. We also might consider Random Forests as an ensemble of trees (bagging instead of boosting) – they are simpler to tune (fewer parameters) but typically less accurate than boosting for structured data. Nonetheless, a Random Forest could be a quick benchmark to gauge the non-linear signal in the data. **Model training considerations:** since we have multiple stations and time series, we might prefer to pool all data and include station as a feature (letting the trees split by station if needed). This way, the model leverages a large sample size but can still differentiate stations. Tree models don’t require feature scaling and can handle missing values (e.g. if some station lacks a certain measurement). We should be cautious to avoid overfitting on station identity or date; techniques in the next bullet (group-aware validation) will guide this. But overall, a gradient boosted tree model, properly regularized (via max_depth, min_child_weight, etc.), will likely capture most of the predictive signal present in our engineered features. If the dataset is large (many days × stations), these models will shine in handling the complexity  (XGBoost is known to perform well on large, complex datasets with non-linear interactions).
-   **Group-Aware Validation (GroupKFold by Station):** Before finalizing any model, we need a validation strategy that reflects the true prediction scenario. In many Kaggle competitions, if the data spans multiple entities (like stations), one must ensure that the validation set is representative and that we’re not leaking information. **GroupKFold** cross-validation is a method where we treat each station as a “group” and ensure that no station’s data appears in both training and validation folds . This is important here because conditions at a given station are not independent day-to-day – if we randomly split days, we might train on a station’s yesterday to predict its tomorrow in validation, which is overly optimistic. By using GroupKFold, we simulate forecasting on unseen stations or at least ensure that when predicting a station’s anomaly, we haven’t trained on that same station’s data in that fold. This is slightly conservative if in reality all stations are known, but it protects against overfitting station-specific quirks. Moreover, it emphasizes learning general patterns applicable across stations (like general weather responses) rather than memorizing each station. If our goal is strictly to predict the next day for *existing* stations (not new ones), we could alternatively use a time-based split (e.g. train on years 2018-2023, validate on 2024). In fact, a robust approach might combine both: ensure the model can generalize across time and stations. But since the prompt highlights GroupKFold by station, we likely assume multiple stations and we want to avoid any leakage of station-specific climate normals or anomalies. In practice, we will do something like 5-fold GroupKFold, where in each fold, ~20% of stations (all days of them) are held out for validation. This ensures, for example, that the model can predict anomalies at station “X” having seen other stations but not X itself in training. If the model performs well in that scenario, it’s capturing real physical relationships, not just overfitting on each station. We should incorporate this strategy during model tuning and selection. This approach aligns with good Kaggle practice: always validate in a way that mimics the test distribution to avoid a nasty surprise on the leaderboard.
-   **Seasonal or Regional Sub-Models:** One powerful strategy is to build specialized models for different subsets of the data, especially when the relationships might differ significantly. **Seasonal models:** We can split the data by season (or month cluster) – for example, train one model for winter months (DJF) and another for summer (JJA), perhaps also one for the transitional seasons. The rationale is that the drivers of anomalies in winter (e.g. snow cover, radiative cooling) are different from those in summer (convection, monsoon rains). By training separate models, each model can focus on the patterns relevant to that season without being “confused” by the opposite season. For instance, a winter model can devote capacity to features like “Siberian high index, snow flag, heating degree days” while a summer model keys in on “precipitation, humidity, typhoon proximity”. Kaggle competitors often do this when a single model struggles to fit all regimes well. We must ensure that we have enough data in each season to train robustly (7 winters, 7 summers across all stations – likely fine). Similarly, **regional sub-models** could be considered: e.g., one model for coastal stations, one for inland/mountain stations. If the climate differences are stark, this might yield better results. For example, coastal station anomalies might be smaller and more driven by wind direction, whereas inland station anomalies might be larger and more driven by local radiative effects. A single model might have trouble if the relationships conflict, but two models can each optimize for their cluster. We could cluster stations (perhaps via k-means on location or on average climate) and train separate models per cluster. In practice, one has to balance complexity – too many specialized models and you risk overfitting or making the system unwieldy. But a handful of season-based or region-based models is manageable. These could then be combined (e.g. if we separate by season, it’s straightforward: use the winter model to predict winter days, etc.). The downside is maintaining multiple models and ensuring you correctly route each new instance to the right model (which is easy if the rule is by date or station). In a Kaggle competition, many top teams try such splits if they notice significantly different error patterns by segment. We would do this if cross-validation shows, say, much higher errors in winter than summer – indicating one model might not be capturing winter dynamics well.
-   **Residual Learning (Two-Stage Models):** We can use a hybrid modeling approach where one model’s output feeds into another. For example, we might first predict tomorrow’s anomaly with a simple model (perhaps a linear climatology-based model or the linear regression), then model the **residual** (the error from the first model) using a more complex model. This is essentially a form of stacking (level-1 model) where the second model learns to correct the biases of the first. Concretely, imagine we use a linear model to capture broad effects (like seasonal average, persistence, etc.). The linear model might predict, say, tomorrow will be +1.0°C anomaly but the actual ends up +2.0°C because there was a nonlinear effect (maybe an interaction or threshold the linear model missed). We then train a second model (e.g. XGBoost) on features including those original features plus the linear model’s prediction, to predict the *residual* (+1.0°C error in that example). The second model can focus on complex patterns the first didn’t handle. This **additive correction** often improves accuracy – the linear part ensures basic consistency and the booster part handles nuance. It’s analogous to how boosting itself works (additive trees correcting residuals), but here one of the components is a different model type. Another residual approach: use climatology as baseline. Since our target is anomaly relative to climatology, one baseline is zero (assuming climatology is correct). But if climatology of 7-year isn’t perfect (e.g. maybe a warming trend makes climatology slightly off), we could first adjust for that. However, since anomaly is defined against current climatology, this may be unnecessary. **Stacked Models:** More generally, we can stack different algorithms. For instance, we could train a linear model, a random forest, and a gradient boosting model, and then feed all their predictions as features into a “meta-model” (which could be a simple linear blend or another regressor) . This meta-model learns how to weigh each model’s prediction. This is useful if different models excel in different situations. For example, the linear model might be very good when anomalies are small and conditions are normal (because it won’t overreact), whereas XGBoost might shine in capturing extreme anomaly events (but could overshoot normals). A meta-model could learn to trust XGBoost more when, say, an extreme event flag is present, but trust the linear model in calm weather. This kind of stacking can squeeze extra performance out – however, one must be careful to use proper validation for stacking to avoid overfitting (usually one uses out-of-fold predictions from the first stage to train the second stage). Kaggle winning solutions frequently use stacking/blending with a handful of diverse models to improve robustness.
-   **Blending and Ensembling:** Aside from formal stacking, a simpler approach is **blending** – taking a weighted average (or median) of predictions from multiple models. For example, we could average the predictions of a linear model, a LightGBM model, and an XGBoost model. Ensembles almost always improve generalization performance , as they reduce model-specific errors. The linear model might handle trend biases, one tree model might handle one kind of pattern, another tree model (with different random seed or slightly different features) might handle others. Averaging them tends to cancel out individual over/under-predictions. In Kaggle competitions, ensembling is a go-to strategy to climb the leaderboard once individual models are optimized. One could also do an **ensemble by time**: e.g., use Model A for winter (as above) and Model B for summer – that’s a form of ensemble where the selection weight is 100% based on season. Or use a simple heuristic blend: if the linear and tree models differ a lot, perhaps take into account uncertainty. But typically, a fixed weighted average (with weights determined via cross-val or a small optimization) works well. We might discover that an ElasticNet and a LightGBM complement each other – the former is very good at capturing linear effects of many features, while the latter picks up non-linear interactions. Blending them could yield better accuracy than either alone. **Note:** Ensembling increases computational cost and complexity, but since Kaggle submissions usually only worry about offline inference time, this is acceptable. We would just need to ensure our ensemble doesn’t overfit (hence we rely on cross-validated performance when choosing ensemble weights). A practical approach: train 5-fold models of LightGBM (to get a stable average prediction), similarly for XGBoost, etc., then average their outputs. This also implicitly smooths out variance. Overall, the motto is “**don’t put all your eggs in one model**” – combining models can leverage their individual strengths and mitigate weaknesses .





In summary, our modeling strategy would likely involve trying a few algorithms (linear, boosted trees, maybe a neural network if time permits, though tabular NNs often underperform GBMs) and using rigorous validation (GroupKFold, temporal split) to select the best approach or blend. We anticipate that an ensemble of a regularized linear model and a gradient boosting model would perform very well – these two often pair effectively. We also take care to respect the data structure (grouped by station, sequential by date) in both training and validating, to ensure our model genuinely learns to forecast anomalies and not just memorize.





### **3. Interpretability and Feature Importance**





Understanding which features and inputs drive the model’s predictions is valuable both for insights and for model trust. In a meteorology context, it also helps validate that the model is learning reasonable relationships (e.g. it should probably pay attention to things like today’s temperature and not random noise). We will employ several interpretability techniques:



-   **Feature Importance (Gain/Split Importance in Trees):** For tree-based models like XGBoost/LightGBM, we can obtain the feature importance either by the number of times a feature was used to split or by the information gain it provided. This gives a quick ranking of which features the model considered most predictive. For instance, we might find that “today’s 3pm temperature” or “24-hr pressure change” are among the top features, indicating these strongly influence the anomaly forecast. However, raw feature importance can be misleading if features are correlated or if the model distributed importance among many features. It’s a starting point, but we prefer more robust measures like SHAP or permutation importance for deeper analysis.
-   **SHAP Values (Shapley Additive Explanations):** **SHAP** is a unified framework for interpreting model predictions that assigns each feature an importance value for a particular prediction . Essentially, SHAP values tell us how much each feature pushed the prediction above or below the average prediction. We will use SHAP to explain our model globally and locally. For example, globally, we can average the absolute SHAP values for each feature to see which features overall have the biggest impact on prediction magnitude. This might reveal that, say, the “yesterday’s anomaly” feature has the largest influence, or that “wind direction (north component)” is very influential in winter predictions. Locally, for a specific day’s prediction, SHAP can show, for instance, that the model predicted a +2.5°C anomaly and that was contributed by features such as “very high max temp today (+1.0°C contribution), low pressure (+0.5°C), etc.”. This is incredibly useful to verify that the model’s reasoning aligns with meteorological expectation. If the model is using weird combinations (like depending heavily on station ID in a way that suggests overfitting), SHAP would highlight that, and we could adjust (e.g. remove or regularize that feature). SHAP provides consistency and theoretically sound attributions . We can visualize SHAP summary plots: for each feature, see the distribution of SHAP values across all samples, which often shows how that feature’s value (low vs high) affects the outcome. For instance, we might see that “high dew point depression” (dry air) often has positive SHAP for anomaly (implying it contributes to colder nights hence negative anomaly? or actually dry air might lead to colder nights -> negative anomaly contribution, depending on how model correlates it). These insights not only build trust but might inspire further feature engineering (if we discover a feature we didn’t include would be helpful).
-   **Permutation Importance:** This is another model-agnostic approach where we randomly shuffle a single feature’s values in the validation set and see how much the model’s error increases. The logic is that if shuffling a feature (destroying its information) causes a big drop in model performance, then the feature was important . We will use permutation importance on our trained model to double-check the ranking of features. Sometimes this can reveal that, say, two features were redundant – if you shuffle one and the model doesn’t lose much accuracy, perhaps another feature is substituting for it. Permutation importance also naturally accounts for interactions: if a feature is only useful in combination with another, shuffling it breaks that combo and hurts performance, so it will still get flagged as important. We might run this station-wise or overall. For example, permutation might show that “station latitude” is not important (perhaps because station-specific effect is better captured by station ID which we included), whereas “min temp of the day” is very important (because shuffling it ruins the model’s ability to get nighttime cooling info). We will ensure to do this in a cross-validated way to avoid variance.
-   **Hourly Contributions Analysis:** Because our input includes hourly readings, we can investigate which hours are most informative for predicting the anomaly. One way is to group features by hour – for instance, treat “temp at 00Z, 01Z, …, 23Z” each as features and look at their learned importances. We might find that certain times of day matter more. Intuitively, the **early morning hours** might be quite predictive of the day’s anomaly: e.g. if even by sunrise the temperature is already above normal, that day is likely to end up warmer than normal (and vice versa). The model might thus rely strongly on, say, 6 AM temperature. Alternatively, **afternoon conditions** could indicate the air mass characteristics that will carry into next day. By examining feature importance for each hour’s temperature or by looking at SHAP values per hour feature, we can rank the hours. If, for example, we see that “yesterday 9 PM temperature” feature has high importance, it suggests the model uses how quickly it cooled last evening as a signal for next day. This kind of insight can be turned into more features (maybe computing cooling rates explicitly). We should also compare importance of different variables: is temperature itself most important (likely yes), but maybe dew point or pressure come close during certain seasons. If our model uses something like station pressure a lot, that tells us pressure is a strong proxy for anomaly (which makes sense: high pressure correlates with radiative cooling, etc.).
-   **Partial Dependence and ICE (Individual Conditional Expectation) Plots:** For key features, we can plot partial dependence to see the marginal effect on the prediction. For instance, a partial dependence plot for “today’s max temp anomaly” (today’s max minus normal max) versus “tomorrow’s mean anomaly” might show a roughly linear increasing relationship – confirming persistence. Or a PDP for “snow flag” might show a clear drop (model predicts significantly negative anomaly when snow flag = 1). These plots help verify that model behavior aligns with expectations (monotonic where it should be, etc.). They are also useful for communication – showing stakeholders that, say, “when yesterday was very cold, the model correspondingly lowers tomorrow’s forecast”.
-   **Model Coefficients (for linear models):** If we include a linear model in our arsenal, its coefficients themselves are an interpretability tool. We can examine the fitted weights for each feature in a Ridge or Lasso model. For example, we might see the Lasso picked “mean temp today” with coefficient 0.5 (meaning for each 1°C today above normal, it predicts +0.5°C anomaly tomorrow, holding other factors constant) and perhaps “precipitation flag” has a coefficient -0.3 (rain today tends to cool tomorrow slightly). Those numbers give direct insight into the assumed linear relationship. However, because we expect interactions and non-linearity, the linear model might not capture everything, but it’s still informative as a first-order effect measure.





Using these interpretability techniques, we will evaluate which features contribute most to predictive power and why. For example, we anticipate features like “today’s average temperature” and “yesterday’s anomaly” will rank high, indicating persistence of anomalies. We may discover that certain *compound* features we created (like an index mixing wind and temp) weren’t actually used by the model – a sign those might be redundant. This feedback can streamline our feature set. Additionally, interpretability ensures that if we deploy this model or present it, we can justify its predictions with meteorological reasoning (“The model predicts a cold anomaly because it detected snow on the ground and clear skies, which aligns with known cooling effects  .”). In a competition write-up, demonstrating such understanding can also be valuable.



**Practical Emphasis:** Ultimately, our approach is geared towards a Kaggle-style competition, meaning we prioritize whatever improves validation scores while grounding our decisions in domain knowledge. We engineered features capturing known climate influences (Part 1 factors) – this adds signal that a purely automated approach might miss. We then apply state-of-the-art modeling practices (ensemble of linear and boosting models, careful CV, possibly stacking) to maximize accuracy. Throughout, we maintain interpretability to verify that the model’s behavior makes sense (using tools like SHAP which offer a unified explanation framework ). By combining scientific understanding with machine learning prowess, we aim to build a model that not only scores well but is also explainable and physically plausible – a critical advantage in forecasting competitions and real-world usage alike.



**Sources:**



1.  WMO (2024) – El Niño impacts on global and regional temperatures  
2.  NOAA/Climate.gov – Explanation of polar vortex and Arctic Oscillation effects  
3.  EPA – Urban heat island effect increases urban temperatures by 1–7°F (day) and 2–5°F (night) 
4.  DTN/Weather – Snow cover reflects ~80–90% of sunlight and cools surface, leading to lower temperatures  
5.  GeeksforGeeks – Regularized regression (Ridge/Lasso) helps avoid overfitting and improve predictive performance 
6.  Medium (Hey Amit) – XGBoost handles non-linear interactions well and often outperforms linear models on complex data 
7.  Kaggle Forum – GroupKFold ensures the same group (station) isn’t in both train and test, preventing leakage 
8.  Kaggle Blog – Ensemble learning improves performance by combining models (stacking/blending)  
9.  Lundberg & Lee (2017) – SHAP assigns each feature an importance value for a prediction, providing a unified interpretability method 
10.  scikit-learn Docs – Permutation importance is the drop in model score when a feature is randomly shuffled, measuring importance 